{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "from scripts.utils import reduce_mem_usage\n",
    "from scripts.anomaly import anomaly_detector\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import h5py\n",
    "import ghalton\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "NO_WEATHER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\", parse_dates=[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20216100 entries, 0 to 20216099\n",
      "Data columns (total 4 columns):\n",
      "building_id      int64\n",
      "meter            int64\n",
      "timestamp        datetime64[ns]\n",
      "meter_reading    float64\n",
      "dtypes: datetime64[ns](1), float64(1), int64(2)\n",
      "memory usage: 616.9 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_metadata = pd.read_csv(\"data/building_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1449 entries, 0 to 1448\n",
      "Data columns (total 6 columns):\n",
      "site_id        1449 non-null int64\n",
      "building_id    1449 non-null int64\n",
      "primary_use    1449 non-null object\n",
      "square_feet    1449 non-null int64\n",
      "year_built     675 non-null float64\n",
      "floor_count    355 non-null float64\n",
      "dtypes: float64(2), int64(3), object(1)\n",
      "memory usage: 68.0+ KB\n"
     ]
    }
   ],
   "source": [
    "building_metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in year_built: 774\n"
     ]
    }
   ],
   "source": [
    "print(f\"Missing values in year_built: {np.sum(building_metadata.year_built.isna())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in floor_count: 1094\n"
     ]
    }
   ],
   "source": [
    "print(f\"Missing values in floor_count: {np.sum(building_metadata.floor_count.isna())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_metadata.drop([\"year_built\",\"floor_count\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train = pd.read_csv(\"data/weather_train.csv\", parse_dates=[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139773 entries, 0 to 139772\n",
      "Data columns (total 9 columns):\n",
      "site_id               139773 non-null int64\n",
      "timestamp             139773 non-null datetime64[ns]\n",
      "air_temperature       139718 non-null float64\n",
      "cloud_coverage        70600 non-null float64\n",
      "dew_temperature       139660 non-null float64\n",
      "precip_depth_1_hr     89484 non-null float64\n",
      "sea_level_pressure    129155 non-null float64\n",
      "wind_direction        133505 non-null float64\n",
      "wind_speed            139469 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(7), int64(1)\n",
      "memory usage: 9.6 MB\n"
     ]
    }
   ],
   "source": [
    "weather_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in cloud_coverage: 69173\n"
     ]
    }
   ],
   "source": [
    "print(f\"Missing values in cloud_coverage: {weather_train.cloud_coverage.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train.drop([\"cloud_coverage\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "merges the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NO_WEATHER:\n",
    "    train_data = pd.merge(train, building_metadata, how=\"left\", on=[\"building_id\"])\n",
    "else:\n",
    "    train_data = (pd.merge(train, building_metadata, how=\"left\", on=[\"building_id\"])\n",
    "                  .merge(weather_train, how=\"left\", on=[\"timestamp\",\"site_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "removes anomal behavior for `site_id=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cut = train_data.query(\"site_id == 0\")\n",
    "ts_uid_values = train_data_cut.loc[:, [\"building_id\", \"meter\"]].drop_duplicates()\n",
    "delete_idx = pd.Index(np.array([], dtype=int))\n",
    "for i,row in ts_uid_values.iterrows():\n",
    "    ts = train_data_cut.query(\"building_id == @row.building_id & meter == @row.meter\")\n",
    "    if ts.query(\"timestamp < '2016-05-20 18:00:00'\").meter_reading.quantile(0.75) == 0:\n",
    "        delete_idx = delete_idx.union(ts.query(\"timestamp < '2016-05-20 18:00:00'\").index)  \n",
    "        \n",
    "train_data.drop(delete_idx, axis=0, inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19871167 entries, 0 to 19871166\n",
      "Data columns (total 7 columns):\n",
      "building_id      int64\n",
      "meter            int64\n",
      "timestamp        datetime64[ns]\n",
      "meter_reading    float64\n",
      "site_id          int64\n",
      "primary_use      object\n",
      "square_feet      int64\n",
      "dtypes: datetime64[ns](1), float64(1), int64(4), object(1)\n",
      "memory usage: 1.0+ GB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "removes anormal behavior for `building_id=363` before `2016-07-25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = train_data.query(\"building_id == 363 & meter == 0 & timestamp <= '2016-07-24 23:00:00'\").index\n",
    "train_data.drop(idx, axis=0, inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19866224 entries, 0 to 19866223\n",
      "Data columns (total 7 columns):\n",
      "building_id      int64\n",
      "meter            int64\n",
      "timestamp        datetime64[ns]\n",
      "meter_reading    float64\n",
      "site_id          int64\n",
      "primary_use      object\n",
      "square_feet      int64\n",
      "dtypes: datetime64[ns](1), float64(1), int64(4), object(1)\n",
      "memory usage: 1.0+ GB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_uid_values = train_data.loc[:, [\"building_id\",\"meter\"]].drop_duplicates()\n",
    "outliers_indexes = list()\n",
    "\n",
    "for _,row in ts_uid_values.iterrows():\n",
    "    ts = train_data.query(\"building_id == @row.building_id & meter == @row.meter\")\n",
    "    ts = ts.reset_index()\n",
    "    outliers = anomaly_detector(ts.meter_reading, window_size=48, sigma=5)\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"{len(outliers)} found for building_id=={row.building_id} & meter={row.meter}\")\n",
    "    outliers_indexes.append(ts.loc[[idx for idx,_ in outliers], :].index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 530.49 Mb (50.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_data = reduce_mem_usage(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19866224 entries, 0 to 19866223\n",
      "Data columns (total 7 columns):\n",
      "building_id      int16\n",
      "meter            int8\n",
      "timestamp        datetime64[ns]\n",
      "meter_reading    float32\n",
      "site_id          int8\n",
      "primary_use      object\n",
      "square_feet      int32\n",
      "dtypes: datetime64[ns](1), float32(1), int16(1), int32(1), int8(2), object(1)\n",
      "memory usage: 530.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NO_WEATHER:\n",
    "    train_data.to_hdf(\"data/train_data_nw.h5\", key=\"train_data\")\n",
    "else:\n",
    "    train_data.to_hdf(\"data/train_data.h5\", key=\"train_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\", parse_dates=[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41697600 entries, 0 to 41697599\n",
      "Data columns (total 4 columns):\n",
      "row_id         int64\n",
      "building_id    int64\n",
      "meter          int64\n",
      "timestamp      datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(3)\n",
      "memory usage: 1.2 GB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_test = pd.read_csv(\"data/weather_test.csv\", parse_dates=[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_test.drop([\"cloud_coverage\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NO_WEATHER:\n",
    "    test_data = pd.merge(test, building_metadata, how=\"left\", on=[\"building_id\"])\n",
    "else: \n",
    "    test_data = (pd.merge(test, building_metadata, how=\"left\", on=[\"building_id\"])\n",
    "                 .merge(weather_test, how=\"left\", on=[\"timestamp\",\"site_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1431.57 Mb (43.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "test_data = reduce_mem_usage(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41697600 entries, 0 to 41697599\n",
      "Data columns (total 7 columns):\n",
      "row_id         int32\n",
      "building_id    int16\n",
      "meter          int8\n",
      "timestamp      datetime64[ns]\n",
      "site_id        int8\n",
      "primary_use    object\n",
      "square_feet    int32\n",
      "dtypes: datetime64[ns](1), int16(1), int32(2), int8(2), object(1)\n",
      "memory usage: 1.4+ GB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NO_WEATHER:\n",
    "    test_data.to_hdf(\"data/test_data_nw.h5\", key=\"test_data\") \n",
    "else:\n",
    "    test_data.to_hdf(\"data/test_data.h5\", key=\"test_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = StratifiedKFold(n_splits=4, shuffle=True, random_state=23)\n",
    "valid_indexes = [valid_index for _,valid_index in splitter.split(train_data, train_data['building_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(\"data/valid_sm_skfold_4fold_shuffle.h5\", \"w\")\n",
    "for i,valid_index in enumerate(valid_indexes):\n",
    "    h5f.create_dataset(f'fold{i}', data=valid_indexes[i])\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### custom validation data - by week - with hausdorff sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 4\n",
    "_train_data = train_data.loc[:, [\"building_id\",\"meter\",\"timestamp\"]]\n",
    "ts_uid_values = _train_data.loc[:, [\"building_id\",\"meter\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2380it [03:03, 12.95it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_indexes = [[] for i in range(n_folds)]\n",
    "generator = ghalton.Halton(1)\n",
    "\n",
    "for _,row in tqdm(ts_uid_values.iterrows()):\n",
    "    ts = _train_data.query(\"building_id == @row.building_id & meter == @row.meter\")\n",
    "    ts[\"week\"] = ts.timestamp.dt.week\n",
    "    weeks = ts.week.unique()\n",
    "    n_weeks = len(weeks)\n",
    "    \n",
    "    sequence = np.asarray(generator.get(n_weeks))[:,0]\n",
    "    idx = rankdata(sequence).astype(int)-1\n",
    "\n",
    "    for i,weeks_idx in enumerate(np.array_split(idx, n_folds)):\n",
    "        weeks_by_fold = weeks[weeks_idx]\n",
    "        valid_indexes[i].append(ts.query(\"week in @weeks_by_fold\").index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(\"data/valid_sm_custom_4fold.h5\", \"w\")\n",
    "for i,valid_index in enumerate(valid_indexes):\n",
    "    h5f.create_dataset(f'fold{i}', data=np.concatenate(valid_index))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### custom validation data - by week - with shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)\n",
    "n_folds = 4\n",
    "_train_data = train_data.loc[:, [\"building_id\",\"meter\",\"timestamp\"]]\n",
    "ts_uid_values = _train_data.loc[:, [\"building_id\",\"meter\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2380it [03:53, 10.18it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_indexes = [[] for i in range(n_folds)]\n",
    "\n",
    "for _,row in tqdm(ts_uid_values.iterrows()):\n",
    "    ts = _train_data.query(\"building_id == @row.building_id & meter == @row.meter\")\n",
    "    ts[\"week\"] = ts.timestamp.dt.week\n",
    "    weeks = ts.week.unique()\n",
    "    np.random.shuffle(weeks)\n",
    "    weeks_split = np.array_split(weeks, n_folds)\n",
    "    np.random.shuffle(weeks_split)\n",
    "    \n",
    "    for i,weeks_by_fold in enumerate(weeks_split):\n",
    "        valid_indexes[i].append(ts.query(\"week in @weeks_by_fold\").index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(\"data/valid_sm_custom1_4fold.h5\", \"w\")\n",
    "for i,valid_index in enumerate(valid_indexes):\n",
    "    h5f.create_dataset(f'fold{i}', data=np.concatenate(valid_index))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### custom validation data - by day of year - with shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)\n",
    "n_folds = 4\n",
    "_train_data = train_data.loc[:, [\"building_id\",\"meter\",\"timestamp\"]]\n",
    "ts_uid_values = _train_data.loc[:, [\"building_id\",\"meter\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2380it [03:49, 10.35it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_indexes = [[] for i in range(n_folds)]\n",
    "\n",
    "for _,row in tqdm(ts_uid_values.iterrows()):\n",
    "    ts = _train_data.query(\"building_id == @row.building_id & meter == @row.meter\")\n",
    "    ts[\"dayofyear\"] = ts.timestamp.dt.dayofyear\n",
    "    days = ts.dayofyear.unique()\n",
    "    np.random.shuffle(days)\n",
    "    days_split = np.array_split(days, n_folds)\n",
    "    np.random.shuffle(days_split)\n",
    "    \n",
    "    for i,days_by_fold in enumerate(days_split):\n",
    "        valid_indexes[i].append(ts.query(\"dayofyear in @days_by_fold\").index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(\"data/valid_sm_custom2_4fold.h5\", \"w\")\n",
    "for i,valid_index in enumerate(valid_indexes):\n",
    "    h5f.create_dataset(f'fold{i}', data=np.concatenate(valid_index))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### custom validation data - by day of year - stratified by month - with shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(19)\n",
    "n_folds = 3\n",
    "_train_data = train_data.loc[:, [\"building_id\",\"meter\",\"timestamp\"]]\n",
    "ts_uid_values = _train_data.loc[:, [\"building_id\",\"meter\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2380it [08:46,  4.52it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_indexes = [[] for i in range(n_folds)]\n",
    "\n",
    "for _,row in tqdm(ts_uid_values.iterrows()):\n",
    "    ts = _train_data.query(\"building_id == @row.building_id & meter == @row.meter\")\n",
    "    ts[\"dayofyear\"] = ts.timestamp.dt.dayofyear\n",
    "    ts[\"month\"] = ts.timestamp.dt.month\n",
    "    \n",
    "    for month in ts.month.unique():\n",
    "        ts_cut = ts.query(\"month == @month\")\n",
    "        days = ts_cut.dayofyear.unique()\n",
    "        np.random.shuffle(days)\n",
    "        days_split = np.array_split(days, n_folds)\n",
    "        np.random.shuffle(days_split)\n",
    "        \n",
    "        for i,days_by_fold in enumerate(days_split):\n",
    "            valid_indexes[i].append(ts_cut.query(\"dayofyear in @days_by_fold\").index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(f\"data/valid_sm_custom_{n_folds}fold.h5\", \"w\")\n",
    "for i,valid_index in enumerate(valid_indexes):\n",
    "    h5f.create_dataset(f'fold{i}', data=np.concatenate(valid_index))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### target scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2380it [05:03,  7.83it/s]\n"
     ]
    }
   ],
   "source": [
    "ts_uid_values = (train_data\n",
    "                 .loc[:, [\"building_id\",\"meter\"]]\n",
    "                 .drop_duplicates())\n",
    "scaling_values = list()\n",
    "\n",
    "for _,row in tqdm(ts_uid_values.iterrows()):\n",
    "    ts = train_data.query(\"building_id == @row.building_id & meter == @row.meter\")\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True)\n",
    "    scaler.fit(ts.meter_reading.values.reshape((-1,1)))\n",
    "    scaling_values.append((row.building_id, row.meter, scaler.center_[0], scaler.scale_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler = pd.DataFrame(scaling_values, columns=[\"building_id\", \"meter\", \"center\", \"scale\"])\n",
    "robust_scaler.to_csv(\"data/robust_scaler.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
